High cardinality data stream processing with large states -- Ning Shi(Klaviyo)
  
At Klaviyo, we process more than a billion events daily with spikes as high as 75,000/s on peak days. The workload is growing exponentially year over year. We migrated our legacy event processing pipeline from Python to Flink in 2018 and gained a tremendous amount of performance. At the same time, we reduced our AWS EC2 instances from over 100 nodes to 15. Due to the nature of multi-tenancy and diverse dataset for over a billion user profiles, we spent a lot of engineering effort solving performance bottlenecks specific to handling high cardinality data streams in Flink. In this talk, we would like to share the learnings on using keyed operator states, windowing on high cardinality data, and making Flink production ready. We will also share the journey of moving from a 99% Python culture to Java.

在Klaviyo，我们每天处理超过10亿个事件，高峰日的峰值高达75000/s。工作量逐年呈指数增长。我们在2018年将遗留事件处理管道从python迁移到了flink，并获得了巨大的性能。同时，我们将AWS EC2实例从100多个节点减少到了15个。由于多租户的性质和超过10亿个用户配置文件的不同数据集，我们花费了大量的工程精力来解决Flink中处理高基数数据流所特有的性能瓶颈。在本文中，我们将分享有关使用键控操作符状态、高基数数据窗口化以及使Flink产品就绪的知识。我们也将分享从99% Python文化到Java的旅程。
